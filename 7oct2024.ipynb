{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is multihead attention', 'context': [Document(id='303e09f2-e5c2-4160-a8ad-1407c36a87ed', metadata={'page': 4.0, 'source': 'attention\\\\attention.pdf'}, page_content='MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\\nwhere head i= Attention( QWQ\\ni,KWK\\ni,VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'), Document(id='87a28655-cf85-4e1f-8ada-3ef476e63761', metadata={'page': 4.0, 'source': 'attention\\\\attention.pdf'}, page_content='MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\\nwhere head i= Attention( QWQ\\ni,KWK\\ni,VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'), Document(id='5b273ae5-eb19-4cab-81e6-ae98993d2aaf', metadata={'page': 4.0, 'source': 'attention\\\\attention.pdf'}, page_content='MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\\nwhere head i= Attention( QWQ\\ni,KWK\\ni,VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'), Document(id='1aac3a63-a13b-4de5-9b81-73fdd2437f81', metadata={'page': 3.0, 'source': 'attention\\\\attention.pdf'}, page_content='we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=∑dk\\ni=1qiki, has mean 0and variance dk.\\n4')], 'answer': 'Multihead attention is a technique that involves using multiple parallel attention layers in parallel, each with a reduced dimension, to capture different aspects of the input. It combines the outputs of these parallel layers to produce the final representation.'}\n",
      "\n",
      "Context: [Document(id='303e09f2-e5c2-4160-a8ad-1407c36a87ed', metadata={'page': 4.0, 'source': 'attention\\\\attention.pdf'}, page_content='MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\\nwhere head i= Attention( QWQ\\ni,KWK\\ni,VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'), Document(id='87a28655-cf85-4e1f-8ada-3ef476e63761', metadata={'page': 4.0, 'source': 'attention\\\\attention.pdf'}, page_content='MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\\nwhere head i= Attention( QWQ\\ni,KWK\\ni,VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'), Document(id='5b273ae5-eb19-4cab-81e6-ae98993d2aaf', metadata={'page': 4.0, 'source': 'attention\\\\attention.pdf'}, page_content='MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\\nwhere head i= Attention( QWQ\\ni,KWK\\ni,VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'), Document(id='1aac3a63-a13b-4de5-9b81-73fdd2437f81', metadata={'page': 3.0, 'source': 'attention\\\\attention.pdf'}, page_content='we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=∑dk\\ni=1qiki, has mean 0and variance dk.\\n4')]\n",
      "\n",
      " Main Answer: Multihead attention is a technique that involves using multiple parallel attention layers in parallel, each with a reduced dimension, to capture different aspects of the input. It combines the outputs of these parallel layers to produce the final representation.\n",
      "\n",
      "Document Similarity Search:\n",
      "MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i,KWK\n",
      "i,VWV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "--------------------------------------------\n",
      "MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i,KWK\n",
      "i,VWV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "--------------------------------------------\n",
      "MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i,KWK\n",
      "i,VWV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i∈Rdmodel×dk,WK\n",
      "i∈Rdmodel×dk,WV\n",
      "i∈Rdmodel×dv\n",
      "andWO∈Rhdv×dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "•In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "--------------------------------------------\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q·k=∑dk\n",
      "i=1qiki, has mean 0and variance dk.\n",
      "4\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('config.env')\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "groq_api_key = os.getenv('groq_api_key')\n",
    "os.environ['GEMINI_API_KEY'] = os.getenv('GEMINI_API_KEY')\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name='gemma-7b-it')\n",
    "\n",
    "# Create a prompt template for the chatbot\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Answer the question based on the provided context only.\n",
    "Please provide the most accurate response based on the question.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Questions: {input}\n",
    "\"\"\")\n",
    "\n",
    "def vector_embedding():\n",
    "    # Load documents and create vector embeddings if not already done\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(google_api_key=google_api_key, model=\"models/text-embedding-004\")\n",
    "    loader = PyPDFDirectoryLoader(\"./attention\")\n",
    "    docs = loader.load()\n",
    "    index_name = 'testproject2'\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    final_documents = text_splitter.split_documents(docs)\n",
    "    \n",
    "    vectors = PineconeVectorStore.from_documents(\n",
    "        documents=final_documents,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "# Example usage of the vector embedding function and querying the model\n",
    "if __name__ == \"__main__\":\n",
    "    vectors = vector_embedding()\n",
    "    \n",
    "    # Sample question to ask from the documents\n",
    "    prompt1 = \"What is multihead attention\"\n",
    "    \n",
    "    # Create a document chain and retrieval chain for answering questions\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    retriever = vectors.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    # Invoke the retrieval chain with a sample input\n",
    "    response = retrieval_chain.invoke({'input': prompt1})\n",
    "    \n",
    "    # Print the answer and document context\n",
    "    print(response)\n",
    "    print(\"\\nContext:\",response[\"context\"])\n",
    "    print(\"\\n Main Answer:\", response['answer'])\n",
    "\n",
    "    print(\"\\nDocument Similarity Search:\")\n",
    "    for i,doc in enumerate(response[\"context\"]):\n",
    "        print(doc.page_content)\n",
    "        print(\"--------------------------------------------\")\n",
    "    \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multihead attention is a technique that uses multiple parallel attention heads to capture different aspects of the input. It involves linearly projecting the queries, keys, and values multiple times with different learned linear projections, resulting in parallel attention over different representation subspaces. The outputs from each head are concatenated and projected once again to produce the final values.\n",
      "Chunk 1: Topic - **Topic:** Transformer Network Architecture for Machine Translation\n",
      "Chunk 2: Topic - **Topic:** Advancement in Machine Translation using Recurrent Neural Networks\n",
      "Chunk 3: Topic - **Topic:** Development of Transformer Models for Language Modeling and Machine Translation\n",
      "Chunk 4: Topic - **Topic:** Research improvements through tensor2tensor framework development\n",
      "Chunk 5: Topic - **Topic:** Sequential Computation in Recurrent Models\n",
      "Chunk 6: Topic - **Topic:** Transformer Model and Attention Mechanism in Sequence Modeling\n",
      "Chunk 7: Topic - **Topic:** Attention Mechanisms in Neural Networks\n",
      "Chunk 8: Topic - **Topic:** Self-Attention in Transformer Model\n",
      "Chunk 9: Topic - **Topic: Transformer Model Architecture**\n",
      "\n",
      "The text chunk discusses the architecture of the Transformer model, including its encoder-decoder structure, self-attention mechanisms, and layer architecture.\n",
      "Chunk 10: Topic - **Topic:** Architecture of Encoder and Decoder in a Neural Network Model\n",
      "Chunk 11: Topic - **Topic:** Transformer Model Architecture\n",
      "Chunk 12: Topic - **Topic:** Attention Mechanism in Neural Networks\n",
      "\n",
      "The text chunk discusses the concept of attention in neural networks, specifically focusing on a type called \"Scaled Dot-Product Attention.\"\n",
      "Chunk 13: Topic - **Topic:** Attention Mechanisms in Machine Learning\n",
      "\n",
      "The text chunk focuses on explaining two types of attention mechanisms commonly used in machine learning models: Scaled Dot-Product Attention and Multi-Head Attention.\n",
      "Chunk 14: Topic - **Topic:** Attention Mechanisms in Neural Networks\n",
      "Chunk 15: Topic - **Topic:** Attention Mechanism in Machine Learning Models\n",
      "Chunk 16: Topic - **Topic:** Attention Mechanism in Transformers\n",
      "Chunk 17: Topic - **Topic:** Self-Attention in Encoder-Decoder Models**\n",
      "\n",
      "The text chunk discusses the use of self-attention mechanisms in encoder-decoder models for sequence-to-sequence learning.\n",
      "Chunk 18: Topic - **Topic:** Feed-forward Networks and Embeddings in Sequence Transduction Models\n",
      "Chunk 19: Topic - **Topic:** Embeddings and Positional Encodings in Sequence Transduction Models\n",
      "Chunk 20: Topic - **Topic:** Positional Encodings in Neural Networks\n",
      "Chunk 21: Topic - **Topic:** Positional Encoding in Self-Attention Models\n",
      "Chunk 22: Topic - **Topic:** Evaluating Self-Attention Layers for Sequence Transduction\n",
      "Chunk 23: Topic - **Topic:** Computational Complexity of Attention-Based Sequence Models\n",
      "Chunk 24: Topic - **Topic:** Computational efficiency in machine translation\n",
      "Chunk 25: Topic - **Topic:** Efficiency of Convolutional Layers in Neural Networks\n",
      "Chunk 26: Topic - **Topic:** Attention-Based Models for Machine Translation\n",
      "Chunk 27: Topic - **Topic: Machine Translation Model Training**\n",
      "\n",
      "The text chunk discusses the training process and hardware specifications for a machine translation model.\n",
      "Chunk 28: Topic - **Topic:** Training Hyperparameters and Regularization Techniques for a Machine Learning Model\n",
      "Chunk 29: Topic - **Topic:** Performance and Cost of Transformer-Based Machine Translation Models\n",
      "Chunk 30: Topic - **Topic:** Transformer Model Performance on WMT 2014 Translation Task\n",
      "Chunk 31: Topic - **Topic:** Transformer Model Hyperparameter Optimization and Evaluation\n",
      "Chunk 32: Topic - **Topic:** Evaluation of Transformer Model Variations on Translation Performance\n",
      "Chunk 33: Topic - **Topic:** Variations in Transformer Architecture and their Impact on Translation Quality\n",
      "Chunk 34: Topic - **Topic:** Transformer Architecture and its Performance for Sequence Transduction\n",
      "Chunk 35: Topic - **Topic:** Transformer model for Machine Translation\n",
      "Chunk 36: Topic - **Topic:** Acknowledgements in a TensorFlow document\n",
      "Chunk 37: Topic - **Topic: References for Research Papers on Neural Machine Translation and Deep Learning**\n",
      "Chunk 38: Topic - **Topic:** Applications of Deep Learning in Sequence Modeling and Representation Learning\n",
      "Chunk 39: Topic - **Topic:** Research papers on Recurrent Neural Networks (RNNs)\n",
      "Chunk 40: Topic - **Topic:** Research papers on Attention Mechanisms in Natural Language Processing published in 2017.\n",
      "Chunk 41: Topic - **Topic:** Attention-based Neural Machine Translation and Related Research\n",
      "Chunk 42: Topic - **Topic:** Neural Network Research Advancements\n",
      "Chunk 43: Topic - **Topic:** Advances in Neural Information Processing Systems and applications in Machine Translation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('config.env')\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "groq_api_key = os.getenv('groq_api_key')\n",
    "os.environ['GEMINI_API_KEY'] = os.getenv('GEMINI_API_KEY')\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name='gemma-7b-it')\n",
    "\n",
    "# Create a prompt template for answering questions based on context\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Answer the question based on the provided context only.\n",
    "Please provide the most accurate response based on the question.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Questions: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Create a prompt template for assigning topics to text chunks\n",
    "topic_prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Assign a topic to the following text chunk:\n",
    "<chunk>\n",
    "{chunk}\n",
    "</chunk>\n",
    "\"\"\")\n",
    "\n",
    "def vector_embedding():\n",
    "    # Load documents and create vector embeddings if not already done\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(google_api_key=google_api_key, model=\"models/text-embedding-004\")\n",
    "    loader = PyPDFDirectoryLoader(\"./attention\")\n",
    "    docs = loader.load()\n",
    "    index_name = 'testproject2'\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    final_documents = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Initialize Pinecone Vector Store and add documents\n",
    "    vectors = PineconeVectorStore.from_documents(\n",
    "        documents=final_documents,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    \n",
    "    return vectors, final_documents  # Return both vectors and original documents\n",
    "\n",
    "def assign_topics_to_chunks(chunks):\n",
    "    topics = []\n",
    "    for chunk in chunks:\n",
    "        # Format the input correctly for LLM invocation\n",
    "        formatted_input = topic_prompt.format(chunk=chunk)\n",
    "        \n",
    "        # Use the LLM to assign a topic to each chunk using formatted input string\n",
    "        response = llm.invoke(formatted_input)\n",
    "        \n",
    "        # Access the content of the AIMessage object directly\n",
    "        topics.append(response.content)  # Change here to access content attribute directly\n",
    "        \n",
    "    return topics\n",
    "\n",
    "# Example usage of the vector embedding function and querying the model\n",
    "if __name__ == \"__main__\":\n",
    "    vectors, final_documents = vector_embedding()  # Get both vectors and original documents\n",
    "    \n",
    "    # Sample question to ask from the documents\n",
    "    prompt1 = \"What is multihead attention\"\n",
    "    \n",
    "    # Create a document chain and retrieval chain for answering questions\n",
    "    document_chain = create_stuff_documents_chain(llm, document_prompt)\n",
    "    retriever = vectors.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    # Invoke the retrieval chain with a sample input\n",
    "    response = retrieval_chain.invoke({'input': prompt1})\n",
    "    \n",
    "    # Print the answer and document context\n",
    "    print(response['answer'])\n",
    "\n",
    "    # Assign topics to each chunk of documents using their page content\n",
    "    chunks = [doc.page_content for doc in final_documents]  # Use final_documents instead of vectors.documents\n",
    "    topics = assign_topics_to_chunks(chunks)\n",
    "    \n",
    "    # Print assigned topics for each chunk\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"Chunk {i+1}: Topic - {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multihead attention is a technique used in the Transformer model that involves combining the outputs of multiple attention heads to enhance the representation of the input. Each attention head focuses on a different aspect of the input, and the combined outputs provide a richer representation.\n",
      "\n",
      "2nd chunk topic:- **Model Performance**\n",
      "\n",
      "The text chunk discusses the performance of a new machine translation model in comparison to existing models.\n",
      "Chunk 1: Topic - **Attention-based models**\n",
      "Chunk 2: Topic - **Model Performance**\n",
      "\n",
      "The text chunk discusses the performance of a new machine translation model in comparison to existing models.\n",
      "Chunk 3: Topic - **Model Development**\n",
      "\n",
      "The text chunk discusses the development of various models related to language processing.\n",
      "Chunk 4: Topic - **Tensor2tensor development**\n",
      "Chunk 5: Topic - **Sequence Modeling**\n",
      "\n",
      "The topic focuses on the challenges and techniques related to modeling sequential data.\n",
      "Chunk 6: Topic - **Topic:** Attention-based models\n",
      "Chunk 7: Topic - **Topic:** Attention mechanism\n",
      "Chunk 8: Topic - **Transformer**\n",
      "\n",
      "The topic describes the Transformer model, which relies solely on self-attention for representation.\n",
      "Chunk 9: Topic - **Transformer Architecture**\n",
      "Chunk 10: Topic - **Encoder and Decoder Architecture**\n",
      "Chunk 11: Topic - **Transformer architecture**\n",
      "Chunk 12: Topic - **Attention Mechanism**\n",
      "\n",
      "The text chunk discusses the concept of attention mechanism, specifically focusing on its application in neural networks.\n",
      "Chunk 13: Topic - **Attention Mechanism**\n",
      "Chunk 14: Topic - **Attention mechanism**\n",
      "Chunk 15: Topic - **Attention Mechanism**\n",
      "Chunk 16: Topic - **Attention mechanisms**\n",
      "Chunk 17: Topic - **Encoder-Decoder Attention**\n",
      "Chunk 18: Topic - **Embedded Representations**\n",
      "\n",
      "The text discusses the use of learned embeddings to convert input and output tokens to vectors.\n",
      "Chunk 19: Topic - **Embedding**\n",
      "\n",
      "The topic relates to the use of learned embeddings in sequence transduction models.\n",
      "Chunk 20: Topic - **Positional Encodings**\n",
      "Chunk 21: Topic - **Positional encoding**\n",
      "Chunk 22: Topic - **Sequence transduction**\n",
      "Chunk 23: Topic - **Dependency length**\n",
      "\n",
      "The topic describes the connection between the length of signal paths in a network and the ability to learn long-range dependencies.\n",
      "Chunk 24: Topic - **Topic:** Self-attention neighborhood size\n",
      "Chunk 25: Topic - **Convolutional Layers**\n",
      "Chunk 26: Topic - **Topic:** Self-attention for Machine Translation\n",
      "Chunk 27: Topic - **Dataset**\n",
      "\n",
      "The given text chunk discusses the dataset used for training the models.\n",
      "Chunk 28: Topic - **Learning Rate Schedule**\n",
      "Chunk 29: Topic - **Machine Translation**\n",
      "\n",
      "The provided text discusses the performance of the Transformer model on machine translation tasks.\n",
      "Chunk 30: Topic - **Translation Models**\n",
      "\n",
      "The topic describes a comparison of translation models based on their BLEU scores and training costs.\n",
      "Chunk 31: Topic - **Model Variations**\n",
      "Chunk 32: Topic - **Hardware Configuration**\n",
      "Chunk 33: Topic - **Architecture**\n",
      "\n",
      "The topic of the text chunk is variations in the Transformer architecture.\n",
      "Chunk 34: Topic - **Model quality**\n",
      "\n",
      "The topic of the text chunk is \"Model quality\".\n",
      "Chunk 35: Topic - **Translation**\n",
      "\n",
      "The topic of the given text chunk is **Translation Models**.\n",
      "Chunk 36: Topic - **Authors**\n",
      "\n",
      "The topic assigned to the given text chunk is \"Authors\".\n",
      "Chunk 37: Topic - **Citations**\n",
      "\n",
      "The text chunk lists various research papers related to machine translation and deep learning.\n",
      "Chunk 38: Topic - **Topic: Machine Learning**\n",
      "Chunk 39: Topic - **Topic: Recurrent Neural Networks**\n",
      "Chunk 40: Topic - **Year**\n",
      "\n",
      "The topic of the text chunk is the year 2017, as mentioned at the beginning and repeated in several references.\n",
      "Chunk 41: Topic - **Topic: Attention-based Neural Machine Translation**\n",
      "Chunk 42: Topic - **Neural networks**\n",
      "\n",
      "The text chunk is about research papers and concepts related to neural networks.\n",
      "Chunk 43: Topic - **Papers**\n",
      "\n",
      "The topic assigned to the given text chunk is \"Papers\".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('config.env')\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "groq_api_key = os.getenv('groq_api_key')\n",
    "os.environ['GEMINI_API_KEY'] = os.getenv('GEMINI_API_KEY')\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name='gemma-7b-it')\n",
    "\n",
    "# Create a prompt template for answering questions based on context\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Answer the question based on the provided context only.\n",
    "Please provide the most accurate response based on the question.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Questions: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Create a prompt template for assigning topics to text chunks\n",
    "topic_prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Topic should be of 3 to 4 words only. Assign a topic to the following text chunk:\n",
    "<chunk>\n",
    "{chunk}\n",
    "</chunk>\n",
    "\"\"\")\n",
    "\n",
    "def vector_embedding():\n",
    "    # Load documents and create vector embeddings if not already done\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(google_api_key=google_api_key, model=\"models/text-embedding-004\")\n",
    "    loader = PyPDFDirectoryLoader(\"./attention\")\n",
    "    docs = loader.load()\n",
    "    index_name = 'testproject2'\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    final_documents = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Initialize Pinecone Vector Store and add documents\n",
    "    vectors = PineconeVectorStore.from_documents(\n",
    "        documents=final_documents,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    \n",
    "    return vectors, final_documents  # Return both vectors and original documents\n",
    "\n",
    "def assign_topics_to_chunks(chunks, documents):\n",
    "    topics = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Format the input correctly for LLM invocation\n",
    "        formatted_input = topic_prompt.format(chunk=chunk)\n",
    "        \n",
    "        # Use the LLM to assign a topic to each chunk using formatted input string\n",
    "        response = llm.invoke(formatted_input)\n",
    "        \n",
    "        # Access the content of the AIMessage object directly\n",
    "        topic = response.content\n",
    "        \n",
    "        # Update the metadata of the corresponding document with the assigned topic\n",
    "        documents[i].metadata['topic'] = topic  # Assign topic to document metadata\n",
    "        topics.append(topic)\n",
    "    print(\"\\n2nd chunk topic:-\",documents[1].metadata['topic'])  \n",
    "    return topics\n",
    "\n",
    "# Example usage of the vector embedding function and querying the model\n",
    "if __name__ == \"__main__\":\n",
    "    vectors, final_documents = vector_embedding()  # Get both vectors and original documents\n",
    "    \n",
    "    # Sample question to ask from the documents\n",
    "    prompt1 = \"What is multihead attention\"\n",
    "    \n",
    "    # Create a document chain and retrieval chain for answering questions\n",
    "    document_chain = create_stuff_documents_chain(llm, document_prompt)\n",
    "    retriever = vectors.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    # Invoke the retrieval chain with a sample input\n",
    "    response = retrieval_chain.invoke({'input': prompt1})\n",
    "    \n",
    "    # Print the answer and document context\n",
    "    print(response['answer'])\n",
    "\n",
    "    # Assign topics to each chunk of documents using their page content\n",
    "    chunks = [doc.page_content for doc in final_documents]  # Use final_documents instead of vectors.documents\n",
    "    topics = assign_topics_to_chunks(chunks, final_documents)  # Pass final_documents for metadata update\n",
    "    \n",
    "    # Print assigned topics for each chunk\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"Chunk {i+1}: Topic - {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text does not contain any information regarding \"multihead attention\", so I am unable to provide an answer to the question from the given context.\n",
      "\n",
      "2nd chunk topic:- RNN\n",
      "Chunk 1: Topic - Transformer\n",
      "Chunk 2: Topic - RNN\n",
      "Chunk 3: Topic - **Collaborators**\n",
      "Chunk 4: Topic - Tensor2tensor\n",
      "Chunk 5: Topic - Sequential computation\n",
      "Chunk 6: Topic - Transformer\n",
      "Chunk 7: Topic - Attention\n",
      "Chunk 8: Topic - Transformer\n",
      "Chunk 9: Topic - Transformer\n",
      "Chunk 10: Topic - Layers\n",
      "Chunk 11: Topic - **Architecture**\n",
      "Chunk 12: Topic - Attention\n",
      "Chunk 13: Topic - Attention\n",
      "Chunk 14: Topic - Attention\n",
      "Chunk 15: Topic - Attention\n",
      "Chunk 16: Topic - Attention\n",
      "Chunk 17: Topic - Encoder-decoder\n",
      "Chunk 18: Topic - Feed-forward networks\n",
      "Chunk 19: Topic - Positional Encodings\n",
      "Chunk 20: Topic - Positional Encodings\n",
      "Chunk 21: Topic - Self-Attention\n",
      "Chunk 22: Topic - Dependency length\n",
      "Chunk 23: Topic - Path length\n",
      "Chunk 24: Topic - Neighborhood\n",
      "Chunk 25: Topic - Convolution\n",
      "Chunk 26: Topic - Training\n",
      "Chunk 27: Topic - Datasets\n",
      "Chunk 28: Topic - Optimizer\n",
      "Chunk 29: Topic - Transformer\n",
      "Chunk 30: Topic - Transformer\n",
      "Chunk 31: Topic - **Variations**\n",
      "Chunk 32: Topic - Attention heads\n",
      "Chunk 33: Topic - Attention key size\n",
      "Chunk 34: Topic - Transformer\n",
      "Chunk 35: Topic - Attention\n",
      "Chunk 36: Topic - Contributors\n",
      "Chunk 37: Topic - References\n",
      "Chunk 38: Topic - Convolution\n",
      "Chunk 39: Topic - Authors\n",
      "Chunk 40: Topic - Year\n",
      "Chunk 41: Topic - Attention\n",
      "Chunk 42: Topic - Neural networks\n",
      "Chunk 43: Topic - Authors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid  # Import uuid for generating unique IDs\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('config.env')\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "groq_api_key = os.getenv('groq_api_key')\n",
    "os.environ['GEMINI_API_KEY'] = os.getenv('GEMINI_API_KEY')\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name='gemma-7b-it')\n",
    "\n",
    "# Create a prompt template for answering questions based on context\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Answer the question based on the provided context only.\n",
    "Please provide the most accurate response based on the question.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Questions: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Create a prompt template for assigning topics to text chunks\n",
    "topic_prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Topic should be of 3 to 4 words only. Topic name should contain only the name. Not '**Topic**' or something similar to this. Assign a topic to the following text chunk:\n",
    "<chunk>\n",
    "{chunk}\n",
    "</chunk>\n",
    "\"\"\")\n",
    "\n",
    "def vector_embedding():\n",
    "    # Load documents and create vector embeddings if not already done\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(google_api_key=google_api_key, model=\"models/text-embedding-004\")\n",
    "    loader = PyPDFDirectoryLoader(\"./attention\")\n",
    "    docs = loader.load()\n",
    "    index_name = 'testproject2'\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    final_documents = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Generate unique IDs for each document if they don't have one\n",
    "    for i, doc in enumerate(final_documents):\n",
    "        if 'id' not in doc.metadata:\n",
    "            doc.metadata['id'] = str(uuid.uuid4())  # Generate a unique ID\n",
    "\n",
    "    # Initialize Pinecone Vector Store and add documents without initial metadata\n",
    "    vectors = PineconeVectorStore.from_documents(\n",
    "        documents=final_documents,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name,\n",
    "    )\n",
    "    \n",
    "    return vectors, final_documents  # Return both vectors and original documents\n",
    "\n",
    "def upsert_metadata(vectors, documents):\n",
    "    \"\"\"Upsert metadata into Pinecone.\"\"\"\n",
    "    ids = [doc.metadata['id'] for doc in documents]\n",
    "    metadatas = [{\"topic\": doc.metadata.get('topic', '')} for doc in documents]\n",
    "    \n",
    "    # Upsert metadata into Pinecone\n",
    "    vectors.add_texts(\n",
    "        texts=[doc.page_content for doc in documents],\n",
    "        metadatas=metadatas,\n",
    "        ids=ids,\n",
    "    )\n",
    "\n",
    "def assign_topics_to_chunks(chunks, documents):\n",
    "    topics = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Format the input correctly for LLM invocation\n",
    "        formatted_input = topic_prompt.format(chunk=chunk)\n",
    "        \n",
    "        # Use the LLM to assign a topic to each chunk using formatted input string\n",
    "        response = llm.invoke(formatted_input)\n",
    "        \n",
    "        # Access the content of the AIMessage object directly\n",
    "        topic = response.content.strip()  # Ensure no leading/trailing whitespace\n",
    "        \n",
    "        # Update the metadata of the corresponding document with the assigned topic\n",
    "        documents[i].metadata['topic'] = topic  # Assign topic to document metadata\n",
    "        \n",
    "        topics.append(topic)\n",
    "    \n",
    "    print(\"\\n2nd chunk topic:-\",documents[1].metadata['topic'])  \n",
    "    return topics\n",
    "\n",
    "# Example usage of the vector embedding function and querying the model\n",
    "if __name__ == \"__main__\":\n",
    "    vectors, final_documents = vector_embedding()  # Get both vectors and original documents\n",
    "    \n",
    "    # Sample question to ask from the documents\n",
    "    prompt1 = \"What is multihead attention\"\n",
    "    \n",
    "    # Create a document chain and retrieval chain for answering questions\n",
    "    document_chain = create_stuff_documents_chain(llm, document_prompt)\n",
    "    retriever = vectors.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    # Invoke the retrieval chain with a sample input\n",
    "    response = retrieval_chain.invoke({'input': prompt1})\n",
    "    \n",
    "    # Print the answer and document context\n",
    "    print(response['answer'])\n",
    "\n",
    "    # Assign topics to each chunk of documents using their page content\n",
    "    chunks = [doc.page_content for doc in final_documents]  # Use final_documents instead of vectors.documents\n",
    "    topics = assign_topics_to_chunks(chunks, final_documents)  # Pass final_documents for metadata update\n",
    "    \n",
    "    # Upsert assigned topics into Pinecone metadata\n",
    "    upsert_metadata(vectors, final_documents)\n",
    "\n",
    "    # Print assigned topics for each chunk\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"Chunk {i+1}: Topic - {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multihead attention is a technique used in machine learning and deep learning models to improve attention between sequences of data points. It involves using multiple parallel attention heads to capture different aspects of the input. In the context of the provided text, it is used in the Transformer model to enhance the ability of the model to capture long-range dependencies in sequence-to-sequence tasks.\n",
      "\n",
      "2nd chunk topic:- RNN\n",
      "Chunk 1: Topic - Transformer\n",
      "Chunk 2: Topic - RNN\n",
      "Chunk 3: Topic - **People**\n",
      "Chunk 4: Topic - Tensor2tensor\n",
      "Chunk 5: Topic - Sequential computation\n",
      "Chunk 6: Topic - Transformer\n",
      "Chunk 7: Topic - Attention\n",
      "Chunk 8: Topic - Transformer\n",
      "Chunk 9: Topic - Architecture\n",
      "Chunk 10: Topic - Layers\n",
      "Chunk 11: Topic - Architecture\n",
      "Chunk 12: Topic - Attention\n",
      "Chunk 13: Topic - Attention\n",
      "Chunk 14: Topic - Attention\n",
      "Chunk 15: Topic - Attention\n",
      "Chunk 16: Topic - MultiHeadAttention\n",
      "Chunk 17: Topic - Encoder-Decoder\n",
      "Chunk 18: Topic - Feed-Forward Network\n",
      "Chunk 19: Topic - Positional Encoding\n",
      "Chunk 20: Topic - Positional Encodings\n",
      "Chunk 21: Topic - Self-Attention\n",
      "Chunk 22: Topic - Dependencies\n",
      "Chunk 23: Topic - Path length\n",
      "Chunk 24: Topic - Neighborhood\n",
      "Chunk 25: Topic - Convolution\n",
      "Chunk 26: Topic - Dataset\n",
      "Chunk 27: Topic - Datasets\n",
      "Chunk 28: Topic - Optimizer\n",
      "Chunk 29: Topic - Transformer\n",
      "Chunk 30: Topic - Transformer\n",
      "Chunk 31: Topic - **ModelVariations**\n",
      "Chunk 32: Topic - **Attention**\n",
      "Chunk 33: Topic - Attention\n",
      "Chunk 34: Topic - Transformer\n",
      "Chunk 35: Topic - Transformer\n",
      "Chunk 36: Topic - Contributors\n",
      "Chunk 37: Topic - References\n",
      "Chunk 38: Topic - Convolution\n",
      "Chunk 39: Topic - Authors\n",
      "Chunk 40: Topic - Year\n",
      "Chunk 41: Topic - Attention\n",
      "Chunk 42: Topic - Neural networks\n",
      "Chunk 43: Topic - Authors\n",
      "Main Topic: **People:**\n",
      "- Authors\n",
      "- Contributors\n",
      "\n",
      "**Transformer:**\n",
      "- Transformer\n",
      "- Attention\n",
      "- Architecture\n",
      "- Layers\n",
      "- Dataset\n",
      "- ModelVariations\n",
      "\n",
      "**RNN:**\n",
      "- Not mentioned\n",
      "\n",
      "**Tensor2tensor:**\n",
      "- Not mentioned\n",
      "\n",
      "**Sequential computation:**\n",
      "- Not mentioned\n",
      "\n",
      "**Dependencies, Path length, Neighborhood, Convolution:**\n",
      "- Not mentioned\n",
      "\n",
      "**Datasets:**\n",
      "- Dataset\n",
      "- Datasets\n",
      "\n",
      "**Optimization:**\n",
      "- Optimizer\n",
      "\n",
      "**Neural networks:**\n",
      "- Neural networks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid  # Import uuid for generating unique IDs\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('config.env')\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "groq_api_key = os.getenv('groq_api_key')\n",
    "os.environ['GEMINI_API_KEY'] = os.getenv('GEMINI_API_KEY')\n",
    "google_api_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "# Initialize the ChatGroq model\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name='gemma-7b-it')\n",
    "\n",
    "# Create a prompt template for answering questions based on context\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Answer the question based on the provided context only.\n",
    "Please provide the most accurate response based on the question.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Questions: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Create a prompt template for assigning topics to text chunks\n",
    "topic_prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Topic should be of 3 to 4 words only. Topic name should contain only the name. Not '**Topic**' or something similar to this. Assign a topic to the following text chunk:\n",
    "<chunk>\n",
    "{chunk}\n",
    "</chunk>\n",
    "\"\"\")\n",
    "\n",
    "def vector_embedding():\n",
    "    # Load documents and create vector embeddings if not already done\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(google_api_key=google_api_key, model=\"models/text-embedding-004\")\n",
    "    loader = PyPDFDirectoryLoader(\"./attention\")\n",
    "    docs = loader.load()\n",
    "    index_name = 'testproject2'\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    final_documents = text_splitter.split_documents(docs)\n",
    "    \n",
    "    # Generate unique IDs for each document if they don't have one\n",
    "    for i, doc in enumerate(final_documents):\n",
    "        if 'id' not in doc.metadata:\n",
    "            doc.metadata['id'] = str(uuid.uuid4())  # Generate a unique ID\n",
    "\n",
    "    # Initialize Pinecone Vector Store and add documents without initial metadata\n",
    "    vectors = PineconeVectorStore.from_documents(\n",
    "        documents=final_documents,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name,\n",
    "    )\n",
    "    \n",
    "    return vectors, final_documents  # Return both vectors and original documents\n",
    "\n",
    "def upsert_metadata(vectors, documents):\n",
    "    \"\"\"Upsert metadata into Pinecone.\"\"\"\n",
    "    ids = [doc.metadata['id'] for doc in documents]\n",
    "    metadatas = [{\"topic\": doc.metadata.get('topic', '')} for doc in documents]\n",
    "    \n",
    "    # Upsert metadata into Pinecone\n",
    "    vectors.add_texts(\n",
    "        texts=[doc.page_content for doc in documents],\n",
    "        metadatas=metadatas,\n",
    "        ids=ids,\n",
    "    )\n",
    "\n",
    "def assign_topics_to_chunks(chunks, documents):\n",
    "    topics = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Format the input correctly for LLM invocation\n",
    "        formatted_input = topic_prompt.format(chunk=chunk)\n",
    "        \n",
    "        # Use the LLM to assign a topic to each chunk using formatted input string\n",
    "        response = llm.invoke(formatted_input)\n",
    "        \n",
    "        # Access the content of the AIMessage object directly\n",
    "        topic = response.content.strip()  # Ensure no leading/trailing whitespace\n",
    "        \n",
    "        # Update the metadata of the corresponding document with the assigned topic\n",
    "        documents[i].metadata['topic'] = topic  # Assign topic to document metadata\n",
    "        \n",
    "        topics.append(topic)\n",
    "    \n",
    "    print(\"\\n2nd chunk topic:-\",documents[1].metadata['topic'])  \n",
    "    return topics\n",
    "\n",
    "def assign_main_topic(topics):\n",
    "    \"\"\"Classify a broader main topic based on assigned topics.\"\"\"\n",
    "    formatted_input = \"Main topic should give only the name, not '**Main Topic**',or something similar to that. Classify the main topic based on the following topics: \" + \", \".join(topics)\n",
    "    response = llm.invoke(formatted_input)\n",
    "    main_topic = response.content.strip()  # Ensure no leading/trailing whitespace\n",
    "    return main_topic\n",
    "\n",
    "def upsert_main_topic(vectors, documents, main_topic):\n",
    "    \"\"\"Upsert the main topic into each document's metadata.\"\"\"\n",
    "    for doc in documents:\n",
    "        doc.metadata['main_topic'] = main_topic  # Assign main topic to document metadata\n",
    "    \n",
    "    # Upsert updated documents back to Pinecone\n",
    "    ids = [doc.metadata['id'] for doc in documents]\n",
    "    metadatas = [{\"topic\": doc.metadata.get('topic', ''), \"main_topic\": doc.metadata.get('main_topic', '')} for doc in documents]\n",
    "    \n",
    "    vectors.add_texts(\n",
    "        texts=[doc.page_content for doc in documents],\n",
    "        metadatas=metadatas,\n",
    "        ids=ids,\n",
    "    )\n",
    "\n",
    "# Example usage of the vector embedding function and querying the model\n",
    "if __name__ == \"__main__\":\n",
    "    vectors, final_documents = vector_embedding()  # Get both vectors and original documents\n",
    "    \n",
    "    # Sample question to ask from the documents\n",
    "    prompt1 = \"What is multihead attention\"\n",
    "    \n",
    "    # Create a document chain and retrieval chain for answering questions\n",
    "    document_chain = create_stuff_documents_chain(llm, document_prompt)\n",
    "    retriever = vectors.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    # Invoke the retrieval chain with a sample input\n",
    "    response = retrieval_chain.invoke({'input': prompt1})\n",
    "    \n",
    "    # Print the answer and document context\n",
    "    print(response['answer'])\n",
    "\n",
    "    # Assign topics to each chunk of documents using their page content\n",
    "    chunks = [doc.page_content for doc in final_documents]\n",
    "    topics = assign_topics_to_chunks(chunks, final_documents)  # Pass final_documents for metadata update\n",
    "    \n",
    "    # Classify a broader 'main topic' based on assigned topics\n",
    "    main_topic = assign_main_topic(topics)\n",
    "\n",
    "    # Upsert assigned topics and main topic into Pinecone metadata\n",
    "    upsert_metadata(vectors, final_documents)\n",
    "    upsert_main_topic(vectors, final_documents, main_topic)\n",
    "\n",
    "    # Print assigned topics for each chunk along with the main topic\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"Chunk {i+1}: Topic - {topic}\")\n",
    "    \n",
    "    print(f\"Main Topic: {main_topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents processed and stored with metadata.\n",
      "{'input': 'What is multihead attention?', 'context': [Document(id='f8ae13e9-f5af-4690-8d1c-277aa0fb3dc6', metadata={'main_topic': 'Attention Mechanisms', 'page': 4.0}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q;K;V ) = softmax(QKT\\npdk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1pdk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\x01k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'), Document(id='7e7fecfc-cba0-4f81-97c0-3f9d7b58fa1b', metadata={'main_topic': 'Attention Mechanisms', 'page': 4.0}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q;K;V ) = softmax(QKT\\npdk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1pdk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\x01k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'), Document(id='5ecc3cb0-ba2a-4df1-b060-b01e97ef5d86', metadata={'main_topic': 'Attention Mechanisms', 'page': 5.0}, page_content='MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO\\nwhere head i= Attention( QWQ\\ni;KWK\\ni;VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni2Rdmodel\\x02dk,WK\\ni2Rdmodel\\x02dk,WV\\ni2Rdmodel\\x02dv\\nandWO2Rhdv\\x02dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\x0fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\\x0fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\x0fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\x001) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0;xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 24]. In the embedding layers, we multiply those weights bypdmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'), Document(id='1410e537-4349-4ce7-9d32-bab56cc4e927', metadata={'main_topic': 'Attention Mechanisms', 'page': 5.0}, page_content='MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO\\nwhere head i= Attention( QWQ\\ni;KWK\\ni;VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni2Rdmodel\\x02dk,WK\\ni2Rdmodel\\x02dk,WV\\ni2Rdmodel\\x02dv\\nandWO2Rhdv\\x02dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\x0fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\\x0fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\x0fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\x001) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0;xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 24]. In the embedding layers, we multiply those weights bypdmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5')], 'answer': 'Multihead attention is a technique used in attention-based models to improve the performance of sequence-to-sequence tasks. It involves dividing the attention process into multiple parallel attention heads, each with a different linear projection of the queries, keys, and values. This allows the model to attend to different parts of the input sequence from different perspectives, resulting in improved representation learning and better performance on sequence-to-sequence tasks.'}\n",
      "\n",
      "Context: [Document(id='f8ae13e9-f5af-4690-8d1c-277aa0fb3dc6', metadata={'main_topic': 'Attention Mechanisms', 'page': 4.0}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q;K;V ) = softmax(QKT\\npdk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1pdk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\x01k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'), Document(id='7e7fecfc-cba0-4f81-97c0-3f9d7b58fa1b', metadata={'main_topic': 'Attention Mechanisms', 'page': 4.0}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention(Q;K;V ) = softmax(QKT\\npdk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1pdk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q\\x01k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'), Document(id='5ecc3cb0-ba2a-4df1-b060-b01e97ef5d86', metadata={'main_topic': 'Attention Mechanisms', 'page': 5.0}, page_content='MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO\\nwhere head i= Attention( QWQ\\ni;KWK\\ni;VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni2Rdmodel\\x02dk,WK\\ni2Rdmodel\\x02dk,WV\\ni2Rdmodel\\x02dv\\nandWO2Rhdv\\x02dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\x0fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\\x0fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\x0fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\x001) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0;xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 24]. In the embedding layers, we multiply those weights bypdmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'), Document(id='1410e537-4349-4ce7-9d32-bab56cc4e927', metadata={'main_topic': 'Attention Mechanisms', 'page': 5.0}, page_content='MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO\\nwhere head i= Attention( QWQ\\ni;KWK\\ni;VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni2Rdmodel\\x02dk,WK\\ni2Rdmodel\\x02dk,WV\\ni2Rdmodel\\x02dv\\nandWO2Rhdv\\x02dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n\\x0fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n\\x0fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n\\x0fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to \\x001) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0;xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 24]. In the embedding layers, we multiply those weights bypdmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5')]\n",
      "\n",
      "Main Answer: Multihead attention is a technique used in attention-based models to improve the performance of sequence-to-sequence tasks. It involves dividing the attention process into multiple parallel attention heads, each with a different linear projection of the queries, keys, and values. This allows the model to attend to different parts of the input sequence from different perspectives, resulting in improved representation learning and better performance on sequence-to-sequence tasks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables if needed\n",
    "load_dotenv('config.env')\n",
    "\n",
    "# Initialize the sentence transformer model for topic identification\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Pre-defined topics (for embedding comparison)\n",
    "predefined_topics = [\n",
    "    \"Transformer Architecture\",\n",
    "    \"Attention Mechanisms\",\n",
    "    \"Neural Networks\",\n",
    "    \"Machine Translation\",\n",
    "    \"Deep Learning\",\n",
    "    \"Natural Language Processing\"\n",
    "]\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Extract text from each page of the PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text_by_page = [page.extract_text() for page in reader.pages]\n",
    "    return text_by_page\n",
    "\n",
    "def identify_main_topics(text_by_page):\n",
    "    main_topics = []\n",
    "    \n",
    "    # Encode predefined topics for comparison\n",
    "    predefined_embeddings = model.encode(predefined_topics, convert_to_tensor=True)\n",
    "    \n",
    "    for i, text in enumerate(text_by_page):\n",
    "        if text:  # Check if text extraction was successful\n",
    "            # Encode the page text\n",
    "            text_embedding = model.encode(text, convert_to_tensor=True)\n",
    "            \n",
    "            # Compute cosine similarities with predefined topics\n",
    "            cosine_scores = util.pytorch_cos_sim(text_embedding, predefined_embeddings)[0]\n",
    "            main_topic_index = cosine_scores.argmax().item()  # Get index of highest similarity\n",
    "            \n",
    "            main_topics.append((i + 1, predefined_topics[main_topic_index]))  # Store page number and identified topic\n",
    "        else:\n",
    "            main_topics.append((i + 1, \"No text found\"))\n",
    "    \n",
    "    return main_topics\n",
    "\n",
    "def create_documents_with_metadata(text_by_page, topics):\n",
    "    documents = []\n",
    "    for i, text in enumerate(text_by_page):\n",
    "        doc_metadata = {\n",
    "            'page': i + 1,\n",
    "            'main_topic': topics[i][1],\n",
    "        }\n",
    "        documents.append(Document(page_content=text, metadata=doc_metadata))\n",
    "    return documents\n",
    "\n",
    "def vector_embedding(documents):\n",
    "    # Assuming embeddings and Pinecone setup is done here\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(google_api_key=os.getenv('GEMINI_API_KEY'), model=\"models/text-embedding-004\")\n",
    "    \n",
    "    index_name = 'testproject4'\n",
    "    \n",
    "    vectors = PineconeVectorStore.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "# Initialize ChatGroq model for querying\n",
    "groq_api_key = os.getenv('groq_api_key')\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name='gemma-7b-it')\n",
    "\n",
    "# Create a prompt template for answering questions based on context\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\" \n",
    "Answer the question based on the provided context only.\n",
    "Please provide the most accurate response based on the question.                                                  \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Questions: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = 'C:/python prac/langchain/attention.pdf'  \n",
    "    text_by_page = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Identify main topics for each page using pre-trained models\n",
    "    topics = identify_main_topics(text_by_page)\n",
    "    documents_with_metadata = create_documents_with_metadata(text_by_page, topics)\n",
    "    \n",
    "    # Create vector embeddings and store them in Pinecone\n",
    "    vectors = vector_embedding(documents_with_metadata)\n",
    "\n",
    "    print(\"Documents processed and stored with metadata.\")\n",
    "\n",
    "    # Sample question to ask from the documents\n",
    "    prompt1 = \"What is multihead attention?\"\n",
    "    \n",
    "    # Create a document chain and retrieval chain for answering questions\n",
    "    document_chain = create_stuff_documents_chain(llm, prompt_template)\n",
    "    retriever = vectors.as_retriever()\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    # Invoke the retrieval chain with a sample input\n",
    "    response = retrieval_chain.invoke({'input': prompt1})\n",
    "    \n",
    "    # Print the answer and document context\n",
    "    print(response)\n",
    "    print(\"\\nContext:\", response[\"context\"])\n",
    "    print(\"\\nMain Answer:\", response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Similarity Search:\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q;K;V ) = softmax(QKT\n",
      "pdk)V (1)\n",
      "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of1pdk. Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q\u0001k=Pdk\n",
      "i=1qiki, has mean 0and variance dk.\n",
      "4\n",
      "--------------------------------------------\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each bypdk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q;K;V ) = softmax(QKT\n",
      "pdk)V (1)\n",
      "The two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of1pdk. Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dkthe two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk[3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients4. To counteract this effect, we scale the dot products by1pdk.\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "4To illustrate why the dot products get large, assume that the components of qandkare independent random\n",
      "variables with mean 0and variance 1. Then their dot product, q\u0001k=Pdk\n",
      "i=1qiki, has mean 0and variance dk.\n",
      "4\n",
      "--------------------------------------------\n",
      "MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i;KWK\n",
      "i;VWV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i2Rdmodel\u0002dk,WK\n",
      "i2Rdmodel\u0002dk,WV\n",
      "i2Rdmodel\u0002dv\n",
      "andWO2Rhdv\u0002dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\u000fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "\u000fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "\u000fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information ﬂow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to \u00001) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0;xW 1+b1)W2+b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\n",
      "dff= 2048 .\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [ 24]. In the embedding layers, we multiply those weights bypdmodel.\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "5\n",
      "--------------------------------------------\n",
      "MultiHead( Q;K;V ) = Concat(head 1;:::;head h)WO\n",
      "where head i= Attention( QWQ\n",
      "i;KWK\n",
      "i;VWV\n",
      "i)\n",
      "Where the projections are parameter matrices WQ\n",
      "i2Rdmodel\u0002dk,WK\n",
      "i2Rdmodel\u0002dk,WV\n",
      "i2Rdmodel\u0002dv\n",
      "andWO2Rhdv\u0002dmodel.\n",
      "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use\n",
      "dk=dv=dmodel=h= 64 . Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\u000fIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "\u000fThe encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "\u000fSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information ﬂow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to \u00001) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0;xW 1+b1)W2+b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\n",
      "dff= 2048 .\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [ 24]. In the embedding layers, we multiply those weights bypdmodel.\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "5\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDocument Similarity Search:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(doc.page_content)\n",
    "    print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents processed and stored with metadata.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # Ensure this is imported\n",
    "\n",
    "# Load environment variables if needed\n",
    "load_dotenv('config.env')\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Extract text from each page of the PDF and concatenate into a single string\n",
    "    reader = PdfReader(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        full_text += page.extract_text() + \"\\n\"  # Add newline for separation\n",
    "    return full_text\n",
    "\n",
    "def chunk_text_semantically(text, embedding_function, max_chunk_length=512):\n",
    "    # Initialize the Semantic Chunker with the provided embedding function\n",
    "    chunker = SemanticChunker(embedding_function)\n",
    "    \n",
    "    # Wrap text in a list and create documents\n",
    "    documents = chunker.create_documents([text])  # Wrap text in a list\n",
    "    \n",
    "    # Filter out chunks that exceed max length\n",
    "    semantic_chunks = [d.page_content for d in documents if len(d.page_content) <= max_chunk_length]\n",
    "    \n",
    "    return semantic_chunks\n",
    "\n",
    "def create_documents_with_metadata(chunks):\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc_metadata = {\n",
    "            'chunk': i + 1,\n",
    "        }\n",
    "        documents.append(Document(page_content=chunk, metadata=doc_metadata))\n",
    "    return documents\n",
    "\n",
    "def vector_embedding(documents):\n",
    "    # Assuming embeddings and Pinecone setup is done here\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(google_api_key=os.getenv('GEMINI_API_KEY'), model=\"models/text-embedding-004\")\n",
    "    \n",
    "    index_name = 'testproject2'\n",
    "    \n",
    "    vectors = PineconeVectorStore.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        index_name=index_name\n",
    "    )\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "# Initialize ChatGroq model for querying\n",
    "groq_api_key = os.getenv('groq_api_key')\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name='gemma-7b-it')\n",
    "\n",
    "# Create a prompt template for answering questions based on context\n",
    "prompt_template = ChatPromptTemplate.from_template(\"\"\" \n",
    "Answer the question based on the provided context only.\n",
    "Please provide the most accurate response based on the question.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Questions: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = 'C:/python prac/langchain/attention.pdf'  \n",
    "    full_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Initialize embedding function for semantic chunking\n",
    "    embedding_function = GoogleGenerativeAIEmbeddings(google_api_key=os.getenv('GEMINI_API_KEY'), model=\"models/text-embedding-004\")\n",
    "    \n",
    "    # Chunk the full text semantically using the Semantic Chunker with a max length of 512 tokens\n",
    "    semantic_chunks = chunk_text_semantically(full_text, embedding_function, max_chunk_length=512)\n",
    "\n",
    "    documents_with_metadata = create_documents_with_metadata(semantic_chunks)\n",
    "    \n",
    "    # Create vector embeddings and store them in Pinecone\n",
    "    vectors = vector_embedding(documents_with_metadata)\n",
    "\n",
    "    print(\"Documents processed and stored with metadata.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text does not contain any information regarding multihead attention, so I am unable to answer this question from the given context.\n"
     ]
    }
   ],
   "source": [
    " # Sample question to ask from the documents\n",
    "prompt1 = \"What is multihead attention?\"\n",
    "    \n",
    "    # Create a document chain and retrieval chain for answering questions\n",
    "document_chain = create_stuff_documents_chain(llm, prompt_template)\n",
    "retriever = vectors.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    # Invoke the retrieval chain with a sample input\n",
    "response = retrieval_chain.invoke({'input': prompt1})\n",
    "    \n",
    "    # Print the answer and document context\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDocument Similarity Search:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(doc.page_content)\n",
    "    print(\"--------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
